{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GIw5zgSkf7mh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip uninstall -y google-cloud-dataflow\n",
        "pip install --upgrade --force tensorflow_transform apache-beam[gcp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvYMdQBxgGp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_transform.beam import impl as beam_impl\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import apache_beam as beam\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCiL-sUKgUp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "REGION = 'asia-east1'\n",
        "BUCKET = '{BUCKET}'\n",
        "PROJECT = '{PROJECT}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXxbc63JgWkx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Cloud Setup\n",
        "This section is only required if running on cloud"
      ]
    },
    {
      "metadata": {
        "id": "wPdeNmBNnAPj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7LnF9NCdgcZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qafXNRhkgqWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Split Dataset\n",
        "Example uses 80-10-10 split for train, eval and test - change if necessary"
      ]
    },
    {
      "metadata": {
        "id": "cnu1C5csgys9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://dl.dropboxusercontent.com/s/y7lm7aton223abm/spam.csv')[['v1', 'v2']]\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qzhISKR9hXzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "train = df.sample(frac=0.8, random_state=RANDOM_SEED)\n",
        "eval = df.drop(train.index)\n",
        "test = eval.sample(frac=0.5, random_state=RANDOM_SEED)\n",
        "eval = eval.drop(test.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMP4-M0cv3Ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def export_datasets(on_cloud=False):\n",
        "  if on_cloud:\n",
        "    data_dir = 'gs://{bucket}/spam-classification/data/split'.format(bucket=BUCKET)\n",
        "  else:\n",
        "    data_dir = 'data/split'\n",
        "  \n",
        "  train.to_csv(os.path.join(data_dir, 'train.csv'))\n",
        "  eval.to_csv(os.path.join(data_dir, 'eval.csv'))\n",
        "  test.to_csv(os.path.join(data_dir, 'test.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3wqf0DGrjKeR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create Dataset using tf.transform"
      ]
    },
    {
      "metadata": {
        "id": "6mSdUpcFoW0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%writefile requirements.txt\n",
        "tensorflow\n",
        "tensorflow-transform\n",
        "apache-beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DfDQYYAli_Q4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_dataset_path(phase, on_cloud=False):\n",
        "  if on_cloud:\n",
        "    data_dir = 'gs://{bucket}/spam-classification/data/split'.format(bucket=BUCKET)\n",
        "  else:\n",
        "    data_dir = 'data/split'\n",
        "  \n",
        "  if phase == 'train':\n",
        "    dataset_dir = os.path.join(data_dir, 'train*.csv')\n",
        "  elif pahse == 'eval':\n",
        "    dataset_dir = os.path.join(data_dir, 'eval*.csv')\n",
        "  else:\n",
        "    dataset_dir = os.path.join(data_dir, 'test*.csv')\n",
        "    \n",
        "  return dataset_dir\n",
        "    \n",
        "\n",
        "def is_valid(inputs):\n",
        "  try:\n",
        "    text = inputs['text']\n",
        "    spam = inputs['spam']\n",
        "    return (text in ('ham', 'spam')) and (len(text) > 0)\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "def preprocess_tft(inputs):\n",
        "  module_url = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1\"\n",
        "  embed = hub.Module(module_url)\n",
        "  result = {}\n",
        "  result['text_embedding'] = embed([inputs])[0]\n",
        "  result['spam'] = tf.identity(inputs['spam'])\n",
        "  return result\n",
        "\n",
        "\n",
        "def preprocess(on_cloud=False):\n",
        "  import datetime\n",
        "  import os\n",
        "  import tempfile\n",
        "  from apache_beam.io import tfrecordio\n",
        "  from tensorflow_transform.coders import example_proto_coder\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import dataset_schema\n",
        "  from tensorflow_transform.beam import tft_beam_io\n",
        "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
        "  \n",
        "  job_name = 'preprocess-spam-dataset' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
        "  \n",
        "  if on_cloud:\n",
        "    print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
        "    OUTPUT_DIR = 'gs://{bucket}/spam-classification/preproc_tft/'.format(bucket=BUCKET)\n",
        "    import subprocess\n",
        "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
        "  else:\n",
        "    import shutil\n",
        "    print('Launching local job ... hang on')\n",
        "    OUTPUT_DIR = './preproc_tft'\n",
        "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "    \n",
        "  options = {\n",
        "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "    'job_name': job_name,\n",
        "    'project': PROJECT,\n",
        "    'max_num_workers': 24,\n",
        "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
        "    'no_save_main_session': True,\n",
        "    'requirements_file': 'requirements.txt'\n",
        "  }\n",
        "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "  \n",
        "  if on_cloud:\n",
        "    RUNNER = 'DataflowRunner'\n",
        "  else:\n",
        "    RUNNER = 'DirectRunner'\n",
        "    \n",
        "  # set up metadata\n",
        "  raw_data_schema = {\n",
        "      colname: dataset_schema.ColumnSchema(\n",
        "          tf.string, \n",
        "          [], \n",
        "          dataset_schema.FixedColumnRepresentation()\n",
        "      ) \n",
        "      for colname in 'spam,text'.split(',')\n",
        "  }\n",
        "  # raw_data_schema.update({\n",
        "  #     colname: dataset_schema.ColumnSchema(\n",
        "  #         tf.float64, \n",
        "  #         [], \n",
        "  #         dataset_schema.FixedColumnRepresentation()\n",
        "  #     )\n",
        "  #     for colname in 'somecolname,anothercolname'.split(',')\n",
        "  #   })\n",
        "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
        "  \n",
        "  # run Beam\n",
        "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
        "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
        "      # save the raw data metadata\n",
        "      _ = (\n",
        "          raw_data_metadata\n",
        "          | 'write_input_metadata' >> tft_beam_io.WriteMetadata(\n",
        "              os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
        "              pipeline=p))\n",
        "      \n",
        "      # analyze and transform training\n",
        "      raw_train_data = (\n",
        "          p \n",
        "          | 'train_read' >> beam.io.Read(beam.io.ReadFromText(get_dataset_path(phase='train', on_cloud=on_cloud)))\n",
        "          | 'train_filter' >> beam.Filter(is_valid))\n",
        "      \n",
        "      raw_train_dataset = (raw_train_data, raw_data_metadata)\n",
        "      transformed_train_dataset, transform_fn = (\n",
        "          raw_train_dataset | beam_impl.AnalyzeAndTransformDataset(preprocessing_fn=preprocess_tft))\n",
        "      transformed_train_data, transformed_metadata = transformed_train_dataset\n",
        "      \n",
        "      # write transformed training data\n",
        "      _ = (\n",
        "          transformed_train_data\n",
        "          | 'write_train_data' >> tfrecordio.WriteToTFRecord(\n",
        "              os.path.join(OUTPUT_DIR, 'train'),\n",
        "              file_name_suffix='.gz',\n",
        "              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
        "      \n",
        "      # transform eval data\n",
        "      raw_eval_data = (\n",
        "          p\n",
        "          | 'eval_read' >> beam.io.Read(beam.io.ReadFromText(get_dataset_path(phase='eval', on_cloud=on_cloud)))\n",
        "          | 'eval_filter' >> beam.Filter(is_valid))\n",
        "      raw_eval_dataset = (raw_eval_data, raw_data_metadata)\n",
        "      transformed_eval_dataset = (\n",
        "          (raw_eval_dataset, transform_fn) | beam_impl.TransformDataset())\n",
        "      transformed_eval_data, _ = transformed_eval_dataset\n",
        "      \n",
        "      # write eval data\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'write_test_data' >> tfrecordio.tfrecordio.WriteToTFRecord(\n",
        "              os.path.join(OUTPUT_DIR, 'eval'),\n",
        "              file_name_suffix='.gz',\n",
        "              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
        "      \n",
        "      # write transform function for serving\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'write_transform_fn' >> transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata'))\n",
        "      )\n",
        "      \n",
        "preprocess(on_cloud=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MirTJpf9rosD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}