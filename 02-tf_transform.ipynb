{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIw5zgSkf7mh"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force tensorflow_transform apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PvYMdQBxgGp9"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "import apache_beam as beam\n",
    "import config\n",
    "import variables\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXxbc63JgWkx"
   },
   "source": [
    "# Cloud Setup\n",
    "This section is only required if running on cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPdeNmBNnAPj"
   },
   "outputs": [],
   "source": [
    "os.environ['BUCKET'] = config.BUCKET\n",
    "os.environ['PROJECT'] = config.PROJECT\n",
    "os.environ['REGION'] = config.REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LnF9NCdgcZV"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wqf0DGrjKeR"
   },
   "source": [
    "# Create Dataset using tf.transform\n",
    "Much of the code is taken from [a notebook in Google's training data analyst repo](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/feateng/tftransform.ipynb).\n",
    "\n",
    "\n",
    "We will use `tf.transform` (which uses Apache Beam under the hood) for the data transformation pipeline. Doing this has several advantages:\n",
    "1. Same input pipeline can be used for batch and streaming data (just need to change the `beam.io.ReadFromText` line to whichever other file/DB connection).\n",
    "1. Same pipeline code can be used regardless of where it is running (on local machine, on GCP, on a Spark cluster).\n",
    "\n",
    "\n",
    "We need to have the following pre-requisites:\n",
    "1. Have a `requirements.txt` file for the Apache Beam pipeline\n",
    "1. Have a data filtering / validation function (here, it is the `is_valid` function)\n",
    "1. Have a data processing function (here, it is the `preprocess_tft` function)\n",
    "\n",
    "\n",
    "We can then define the pipeline:\n",
    "1. Import required packages for the pipeline\n",
    "1. Prepare the filesystem by deleting leftover files\n",
    "1. Set up the options for the pipeline\n",
    "1. Create metadata about the inputs (input columns and their datatypes) and save it\n",
    "1. Create a pipeline with the desired runner (e.g. `DirectRunner` or `DataflowRunner`)\n",
    "1. Transform raw training data\n",
    "  1. Read in raw training data\n",
    "  1. Analyze and transform the raw training data (produces a transformed dataset **and** a transform function to use on the eval and test datasets)\n",
    "  1. Save the transformed data\n",
    "1. Transform eval and test data\n",
    "  1. Read in raw eval/test data\n",
    "  1. Transform raw eval/test data\n",
    "  1. Save transformed eval/test data\n",
    "1. Save transform function for serving\n",
    "\n",
    "Note: when you read in data, it becomes a `PCollection`. Combining a `PCollection` with `DatasetMetadata` makes it a dataset. A dataset is a `tf.transform` concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mSdUpcFoW0U"
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow\n",
    "tensorflow-transform\n",
    "tensorflow-hub\n",
    "apache-beam\n",
    "python-snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DfDQYYAli_Q4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# keras doesn't support the label_vocabulary argument like canned estimators in tf,estimator,\n",
    "# so we have to reverse engineer the vocab lookup\n",
    "with open('data/misc/labels.txt', 'r') as f:\n",
    "    labels = []\n",
    "    for line in f.readlines():\n",
    "        labels.append(line.strip('\\n'))\n",
    "\n",
    "def decode_csv(row):\n",
    "    try:\n",
    "        split = row.split(config.DELIM)\n",
    "        features = dict(zip(config.RENAMED_COLS, split))\n",
    "        for col in config.STRING_COLS:\n",
    "            features[col] = features[col].strip()\n",
    "        for col in config.NUMERIC_COLS:\n",
    "            features[col] = float(features[col])\n",
    "    except ValueError:\n",
    "        features = {}\n",
    "        for col in config.STRING_COLS:\n",
    "            features[col] = ''.join(['dummy_', col])\n",
    "        for col in config.NUMERIC_COLS:\n",
    "            features[col] = 0.0\n",
    "    finally:\n",
    "        all_cols = config.STRING_COLS + config.NUMERIC_COLS\n",
    "        for key in features.keys():\n",
    "            if key not in all_cols:\n",
    "                features.pop(key)\n",
    "        return features\n",
    "\n",
    "def is_valid(inputs):\n",
    "    try:\n",
    "        text = inputs['text']\n",
    "        spam = inputs['spam']\n",
    "        valid = (spam in ('ham', 'spam')) and (len(text) > 0)\n",
    "        return valid\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def preprocess_tft(inputs):\n",
    "    with tf.Session() as sess:\n",
    "        result = inputs.copy()\n",
    "        # string split returns rank 2 sparse tensor, containing (i) indices of the rows\n",
    "        # and (ii) the integer mapping of each word\n",
    "        tokens = tf.strings.split(inputs[config.TOKENIZE_COL])\n",
    "        tokens = tft.ngrams(tokens, ngram_range=config.NGRAM_RANGE, separator=' ')\n",
    "        indices = tft.compute_and_apply_vocabulary(tokens, top_k=config.MAX_TOKENS)\n",
    "        bow_indices, bow_weight = tft.tfidf(indices, vocab_size=config.MAX_TOKENS+1)\n",
    "        result['bow_indices'] = bow_indices\n",
    "        result['bow_weight'] = bow_weight\n",
    "        \n",
    "        # keras doesn't support the label_vocabulary argument like canned estimators in tf.estimator,\n",
    "        # so we have to reverse engineer the vocab lookup\n",
    "        result[config.LABEL_COL + '_one_hot'] = tft.apply_function(\n",
    "            lambda s: tf.contrib.lookup.string_to_index(s, mapping=labels), \n",
    "            result[config.LABEL_COL])\n",
    "        result[config.LABEL_COL + '_one_hot'] = tf.one_hot(result[config.LABEL_COL + '_one_hot'], depth=variables.N_CLASSES)\n",
    "        result[config.LABEL_COL] = inputs[config.LABEL_COL]\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_dataset_path(phase, on_cloud=False):\n",
    "    if on_cloud:\n",
    "        data_dir = 'gs://{bucket}/spam-classification/data/split'.format(bucket=config.BUCKET)\n",
    "    else:\n",
    "        data_dir = 'data/split'\n",
    "  \n",
    "    if phase == 'train':\n",
    "        dataset_dir = os.path.join(data_dir, 'train*.csv')\n",
    "    elif phase == 'eval':\n",
    "        dataset_dir = os.path.join(data_dir, 'eval*.csv')\n",
    "    else:\n",
    "        dataset_dir = os.path.join(data_dir, 'test*.csv')\n",
    "    \n",
    "    return dataset_dir\n",
    "\n",
    "\n",
    "def preprocess(on_cloud=False):\n",
    "    import datetime\n",
    "    import os\n",
    "    import tempfile\n",
    "    from apache_beam.io import tfrecordio\n",
    "    from tensorflow_transform.coders import example_proto_coder\n",
    "    from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "    from tensorflow_transform.tf_metadata import dataset_schema\n",
    "    from tensorflow_transform.beam import tft_beam_io\n",
    "    from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "    job_name = 'preprocess-for-{project}-'.format(project=config.PROJECT) + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    \n",
    "    # flush all old data before running\n",
    "    if on_cloud:\n",
    "        print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "        OUTPUT_DIR = 'gs://{bucket}/spam-classification/data/tft'.format(bucket=config.BUCKET)\n",
    "        import subprocess\n",
    "        subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "    else:\n",
    "        import shutil\n",
    "        print('Launching local job ... hang on')\n",
    "        OUTPUT_DIR = './data/tft'\n",
    "        shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    \n",
    "    options = {\n",
    "        'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "        'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "        'job_name': job_name,\n",
    "        'project': config.PROJECT,\n",
    "        'max_num_workers': 24,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'requirements_file': 'requirements.txt'\n",
    "    }\n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  \n",
    "    if on_cloud:\n",
    "        RUNNER = 'DataflowRunner'\n",
    "    else:\n",
    "        RUNNER = 'DirectRunner'\n",
    "    \n",
    "    # set up metadata\n",
    "    raw_data_schema = {}\n",
    "    if config.STRING_COLS:\n",
    "        raw_data_schema.update({\n",
    "            colname: dataset_schema.ColumnSchema(\n",
    "                tf.string, \n",
    "                [], \n",
    "                dataset_schema.FixedColumnRepresentation()\n",
    "            ) \n",
    "            for colname in config.STRING_COLS\n",
    "        })\n",
    "    if config.NUMERIC_COLS:\n",
    "        raw_data_schema.update({\n",
    "            colname: dataset_schema.ColumnSchema(\n",
    "                tf.float32,\n",
    "                [],\n",
    "                dataset_schema.FixedColumnRepresentation()\n",
    "            )\n",
    "            for colname in config.NUMERIC_COLS\n",
    "        })\n",
    "    raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "  \n",
    "    # run Beam\n",
    "    with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "        with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "            # save the raw data metadata\n",
    "            _ = (\n",
    "                raw_data_metadata\n",
    "                | 'write_input_metadata' >> tft_beam_io.WriteMetadata(\n",
    "                    os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
    "                    pipeline=p))\n",
    "      \n",
    "            # analyze and transform training\n",
    "            raw_train_data = (\n",
    "                p\n",
    "                | 'train_read' >> beam.io.ReadFromText(get_dataset_path(phase='train', on_cloud=on_cloud))\n",
    "                | 'train_decode' >> beam.Map(decode_csv)\n",
    "                | 'train_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "            raw_train_dataset = (raw_train_data, raw_data_metadata)\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | 'transform_train_data' >> beam_impl.AnalyzeAndTransformDataset(preprocessing_fn=preprocess_tft))\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed training data\n",
    "            _ = (\n",
    "                transformed_train_data\n",
    "                | 'write_train_data' >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, 'train'),\n",
    "                    file_name_suffix='.gz',\n",
    "                    coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
    "\n",
    "            # transform eval data\n",
    "            raw_eval_data = (\n",
    "                p\n",
    "                | 'eval_read' >> beam.io.ReadFromText(get_dataset_path(phase='eval', on_cloud=on_cloud))\n",
    "                | 'eval_decode' >> beam.Map(decode_csv)\n",
    "                | 'eval_filter' >> beam.Filter(is_valid))\n",
    "            raw_eval_dataset = (raw_eval_data, raw_data_metadata)\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn)\n",
    "                | 'transform_eval_data' >> beam_impl.TransformDataset())\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "\n",
    "            # write eval data\n",
    "            _ = (\n",
    "                transformed_eval_data\n",
    "                | 'write_eval_data' >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, 'eval'),\n",
    "                    file_name_suffix='.gz',\n",
    "                    coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
    "\n",
    "            # transform test data\n",
    "            raw_test_data = (\n",
    "                p\n",
    "                | 'test_read' >> beam.io.ReadFromText(get_dataset_path(phase='test', on_cloud=on_cloud))\n",
    "                | 'test_decode' >> beam.Map(decode_csv)\n",
    "                | 'test_filter' >> beam.Filter(is_valid))\n",
    "            raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "            transformed_test_dataset = (\n",
    "                (raw_test_dataset, transform_fn)\n",
    "                | 'transform_test_data' >> beam_impl.TransformDataset())\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # write test data\n",
    "            _ = (\n",
    "                transformed_test_data\n",
    "                | 'write_test_data' >> tfrecordio.WriteToTFRecord(\n",
    "                    os.path.join(OUTPUT_DIR, 'test'),\n",
    "                    file_name_suffix='.gz',\n",
    "                    coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
    "\n",
    "            # write transform function for serving\n",
    "            _ = (\n",
    "                transform_fn\n",
    "                | 'write_transform_fn' >> transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata'))\n",
    "            )\n",
    "      \n",
    "\n",
    "preprocess(on_cloud=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02-tf_transform.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
