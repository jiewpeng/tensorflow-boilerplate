{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejuHKQ9UfSMJ"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tensorflow-transform\n",
    "pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('trainer'):\n",
    "    os.mkdir('trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSkew8KnU3qJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function, division, absolute_import # python 2 compatibility\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "from tensorflow_transform.saved import input_fn_maker, saved_transform_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.beam.tft_beam_io import beam_metadata_io\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_hub as hub\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "import os\n",
    "import config\n",
    "import variables\n",
    "from sepcnn_model import cnn_model_fn\n",
    "print(tf.__version__)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBEdbhATWPmL"
   },
   "source": [
    "# Cloud Setup\n",
    "This section is required only if running on cloud (ML Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHu_5_dqrssE"
   },
   "outputs": [],
   "source": [
    "os.environ['PROJECT'] = config.PROJECT\n",
    "os.environ['BUCKET'] = config.BUCKET\n",
    "os.environ['REGION'] = config.REGION\n",
    "os.environ['TFVERSION'] = config.TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ia9n5ZtEWFks"
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbkA9Vd_tKzU"
   },
   "source": [
    "# Set up Model as a Package\n",
    "\n",
    "## Pre-requisites\n",
    "Data is assumed to have been prepared in the `TFRecords` format with GZIP compression. This gets us the best performance and scalability compared to csv files. The conversion of `csv` to `TFRecords` should be done in the previous notebook, `02-tf_transform.ipynb`\n",
    "\n",
    "## Package Setup\n",
    "We need to set up our model as a package for training and serving.\n",
    "\n",
    "- `model.py` provides the code for data inputs and the model itself\n",
    "- `setup.py` provides metadata about the package\n",
    "- `task.py` sets up the package to be used from the command line, with arguments that specify hyperparameters to the model as well as GCP resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C952m8_sWKJf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/model.py --append\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type, embedding_type, learning_rate,\n",
    "                    hidden_units, dropout,\n",
    "                    l1_regularization_strength, l2_regularization_strength,\n",
    "                    blocks, filters, kernel_size, pool_size):\n",
    "\n",
    "    if embedding_type == 'nnlm':\n",
    "        module_url = 'https://tfhub.dev/google/nnlm-en-dim128/1'\n",
    "        embedding_size = 128\n",
    "    elif embedding_type == 'universal-sentence-encoder':\n",
    "        module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
    "        embedding_size = 512\n",
    "    elif embedding_type == 'elmo':\n",
    "        module_url = 'https://tfhub.dev/google/elmo/2'\n",
    "        embedding_size = 1024\n",
    "    elif embedding_type == 'wikiwords250':\n",
    "        module_url = 'https://tfhub.dev/google/Wiki-words-250/1'\n",
    "        embedding_size = 250\n",
    "    else:\n",
    "        raise InputError('Embedding type must be one of \"nnlm\", \"universal-sentence-encoder\", \"elmo\"')\n",
    "    \n",
    "    if model_type in ('linear', 'dnn-linear-combined'):\n",
    "        bow_indices = tf.feature_column.categorical_column_with_identity('bow_indices', num_buckets=config.MAX_TOKENS+1)\n",
    "        weighted_bow = tf.feature_column.weighted_categorical_column(bow_indices, 'bow_weight')\n",
    "    if model_type in ('dnn', 'dnn-linear-comnbined'):\n",
    "        embedding = hub.text_embedding_column(config.TOKENIZE_COL, module_url, trainable=False)\n",
    "    if model_type in ('cnn', 'rnn'):\n",
    "        pass\n",
    "    \n",
    "    if model_type == 'linear':\n",
    "        feature_columns = [weighted_bow]\n",
    "        \n",
    "        estimator = tf.estimator.LinearClassifier(\n",
    "            feature_columns=feature_columns,\n",
    "            n_classes=variables.N_CLASSES,\n",
    "            label_vocabulary=variables.LABEL_VOCABULARY,\n",
    "            model_dir=model_dir,\n",
    "            optimizer=tf.train.FtrlOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "                l1_regularization_strength=l1_regularization_strength,\n",
    "                l2_regularization_strength=l2_regularization_strength\n",
    "            )\n",
    "        )\n",
    "    elif model_type == 'dnn':\n",
    "        feature_columns = [embedding]\n",
    "        \n",
    "        estimator = tf.estimator.DNNClassifier(\n",
    "            feature_columns=feature_columns,\n",
    "            hidden_units=hidden_units,\n",
    "            n_classes=variables.N_CLASSES,\n",
    "            label_vocabulary=variables.LABEL_VOCABULARY,\n",
    "            model_dir=model_dir,\n",
    "            optimizer=tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "            ),\n",
    "            dropout=dropout\n",
    "        )\n",
    "    elif model_type == 'dnn-linear-combined':\n",
    "        dnn_features = [embedding]\n",
    "        linear_features = [weighted_bow]\n",
    "        \n",
    "        estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "            linear_feature_columns=linear_features,\n",
    "            linear_optimizer=tf.train.FtrlOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "                l1_regularization_strength=l1_regularization_strength,\n",
    "                l2_regularization_strength=l2_regularization_strength\n",
    "            ),\n",
    "            dnn_feature_columns=dnn_features,\n",
    "            dnn_optimizer=tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "            ),\n",
    "            dnn_dropout=dropout,\n",
    "            dnn_hidden_units=hidden_units,\n",
    "            n_classes=variables.N_CLASSES,\n",
    "            label_vocabulary=variables.LABEL_VOCABULARY,\n",
    "            model_dir=model_dir,\n",
    "            batch_norm=True\n",
    "        )\n",
    "    elif model_type == 'sepcnn':\n",
    "        params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'blocks': blocks,\n",
    "            'filters': filters,\n",
    "            'kernel_size': kernel_size,\n",
    "            'dropout_rate': dropout,\n",
    "            'pool_size': pool_size,\n",
    "            'num_classes': variables.N_CLASSES,\n",
    "            'module_url': module_url,\n",
    "            'is_embedding_trainable': False,\n",
    "            'embedding_size': embedding_size\n",
    "        }\n",
    "        \n",
    "        estimator = tf.estimator.Estimator(model_fn=cnn_model_fn, model_dir=model_dir, params=params)\n",
    "    elif model_type == 'rnn':\n",
    "        embed = hub.Module(module_url, trainable=False)\n",
    "        def seq_embed(batch_of_text):\n",
    "            words = tf.string_split(tf.squeeze(batch_of_text))\n",
    "            batch_size = words.dense_shape[0]\n",
    "            dense_words = tf.sparse_to_dense(\n",
    "                sparse_indices=words.indices,\n",
    "                sparse_values=words.values,\n",
    "                default_value='',\n",
    "                output_shape=(batch_size, config.MAX_SEQ_LEN)\n",
    "            )\n",
    "            embeddings = tf.map_fn(lambda token: embed(token), dense_words, dtype=tf.float32)\n",
    "            \n",
    "            return embeddings\n",
    "            \n",
    "        text_input = tf.keras.layers.Input(shape=(config.MAX_SEQ_LEN, embedding_size), name=config.TOKENIZE_COL, dtype=tf.float32)\n",
    "        processed = tf.keras.layers.Lambda(seq_embed)(text_input)\n",
    "        processed = tf.keras.layers.Lambda(seq_embed)(processed)\n",
    "        for unit in hidden_units:\n",
    "            processed = tf.keras.layers.LSTM(unit)(processed)\n",
    "        processed = tf.keras.layers.Dense(128, activation='relu')(processed)\n",
    "        processed = tf.keras.layers.Dropout(dropout)(processed)\n",
    "        output = tf.keras.layers.Dense(variables.N_CLASSES, activation='sigmoid', name='probabilities')(processed)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=text_input, outputs=output)\n",
    "        \n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        estimator = tf.keras.estimator.model_to_estimator(model)\n",
    "        \n",
    "    else:\n",
    "        raise InputErorr('Model type must be one of \"linear\" or \"dnn\"')\n",
    "        \n",
    "    if len(config.PASSTHROUGH_COLS) > 0:\n",
    "        estimator = tf.contrib.estimator.forward_features(estimator, config.PASSTHROUGH_COLS)\n",
    "        \n",
    "    def get_model_fn_with_removed_outputs(estimator):\n",
    "        def _model_fn(features, labels, mode):\n",
    "            config = estimator.config\n",
    "            model_fn_ops = estimator._model_fn(features=features, labels=labels, mode=mode, config=config)\n",
    "            model_fn_ops.predictions['probability'] = tf.reduce_max(model_fn_ops.predictions['probabilities'], axis=-1)\n",
    "            for key in ('logits', 'logistic', 'probabilities', 'class_ids'):\n",
    "                try:\n",
    "                    model_fn_ops.predictions.pop(key)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            return model_fn_ops\n",
    "        return _model_fn\n",
    "    \n",
    "    if model_type != 'sepcnn':\n",
    "        estimator = tf.estimator.Estimator(model_fn=get_model_fn_with_removed_outputs(estimator))\n",
    "    \n",
    "    return estimator\n",
    "        \n",
    "# Serving input function\n",
    "def make_serving_input_fn_for_base64_json(args):\n",
    "    raw_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(args['metadata_path'], 'rawdata_metadata'))\n",
    "    transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "    return input_fn_maker.build_parsing_transforming_serving_input_receiver_fn(\n",
    "        raw_metadata,\n",
    "        transform_savedmodel_dir,\n",
    "        exclude_raw_keys=[config.LABEL_COL]\n",
    "    )\n",
    "\n",
    "def make_serving_input_fn(args):\n",
    "    transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "    \n",
    "    def _input_fn():\n",
    "        feature_placeholders = {\n",
    "            column_name: tf.placeholder(tf.string, [None]) for column_name in config.STRING_COLS\n",
    "        }\n",
    "        feature_placeholders.update({\n",
    "            column_name: tf.placeholder(tf.float32, [None]) for column_name in config.NUMERIC_COLS\n",
    "        })\n",
    "        feature_placeholders.pop(config.LABEL_COL)\n",
    "        \n",
    "        _, features = saved_transform_io.partially_apply_saved_transform(\n",
    "            transform_savedmodel_dir,\n",
    "            feature_placeholders\n",
    "        )\n",
    "        \n",
    "        # so that outputs are consistently in lists\n",
    "        if len(config.PASSTHROUGH_COLS) > 0:\n",
    "            for col in config.PASSTHROUGH_COLS:\n",
    "                features[col] = tf.expand_dims(tf.identity(feature_placeholders[col]), axis=1)\n",
    "        \n",
    "        return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def make_eval_input_fn(args):\n",
    "    transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "    \n",
    "    def _input_fn():\n",
    "        metadata = beam_metadata_io.metadata_io.read_metadata('data/tft/metadata/rawdata_metadata/')\n",
    "        raw_feature_spec = metadata.schema.as_feature_spec()\n",
    "\n",
    "        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[None], name='input_example_tensor')\n",
    "\n",
    "        features = tf.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        \n",
    "        _, transformed_features = saved_transform_io.partially_apply_saved_transform(\n",
    "            transform_savedmodel_dir,\n",
    "            features\n",
    "        )\n",
    "        \n",
    "        receiver_tensors = {'examples': serialized_tf_example}\n",
    "        \n",
    "        return tfma.export.EvalInputReceiver(\n",
    "            features=transformed_features,\n",
    "            receiver_tensors=receiver_tensors,\n",
    "            labels=transformed_features[config.LABEL_COL]\n",
    "        )\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "# training, eval and test input function\n",
    "def read_dataset(args, mode):\n",
    "    batch_size = args['train_batch_size']\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        input_paths = args['train_data_paths']\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        input_paths = args['eval_data_paths']\n",
    "    else:\n",
    "        input_paths = args['test_data_paths']\n",
    "    \n",
    "    transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(args['metadata_path'], 'transformed_metadata'))\n",
    "    \n",
    "    return input_fn_maker.build_training_input_fn(\n",
    "        metadata=transformed_metadata,\n",
    "        file_pattern = (input_paths[0] if len(input_paths) == 1 else input_paths),\n",
    "        training_batch_size=batch_size,\n",
    "        label_keys=[config.LABEL_COL + '_ one_hot' if args['model_type'] in ('rnn', 'cnn') else config.LABEL_COL],\n",
    "        reader=gzip_reader_fn,\n",
    "        randomize_input=(mode == tf.estimator.ModeKeys.TRAIN),\n",
    "        num_epochs=(None if mode == tf.estimator.ModeKeys.TRAIN else 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# create tf.estimator train and evaluate function\n",
    "def train_and_evaluate(args):\n",
    "    # figure out train steps based on no. of epochs, no. of rows in dataset and batch size\n",
    "    tfrecord_options = tf.python_io.TFRecordOptions(compression_type=tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    nrows = sum(\n",
    "        sum(1 for _ in tf.python_io.tf_record_iterator(f, options=tfrecord_options)) \n",
    "        for f in tf.gfile.Glob(args['train_data_paths'])\n",
    "    )\n",
    "    num_epochs = args['num_epochs']\n",
    "    batch_size = args['train_batch_size']\n",
    "    if batch_size > nrows:\n",
    "        batch_size = nrows\n",
    "    max_steps = num_epochs * nrows / batch_size\n",
    "    \n",
    "    # modify according to build_estimator function\n",
    "    estimator = build_estimator(\n",
    "        model_dir=args['model_dir'],\n",
    "        model_type=args['model_type'],\n",
    "        embedding_type=args['embedding_type'],\n",
    "        learning_rate=args['learning_rate'],\n",
    "        hidden_units=args['hidden_units'].split(' '),\n",
    "        dropout=args['dropout'],\n",
    "        l1_regularization_strength=args['l1_regularization_strength'],\n",
    "        l2_regularization_strength=args['l2_regularization_strength'],\n",
    "        blocks=args['blocks'],\n",
    "        filters=args['filters'],\n",
    "        kernel_size=args['kernel_size'],\n",
    "        pool_size=args['pool_size']\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset(args, tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=max_steps\n",
    "    )\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', make_serving_input_fn(args))\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset(args, tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        exporters=exporter\n",
    "    )\n",
    "    \n",
    "    \n",
    "    if args['debug'] == 'True':\n",
    "        \n",
    "        train_spec = tf.estimator.TrainSpec(\n",
    "            input_fn=read_dataset(args, tf.estimator.ModeKeys.TRAIN),\n",
    "            max_steps=5\n",
    "        )\n",
    "        \n",
    "        eval_spec = tf.estimator.EvalSpec(\n",
    "            input_fn=read_dataset(args, tf.estimator.ModeKeys.EVAL),\n",
    "            steps=1,\n",
    "            exporters=exporter\n",
    "        )\n",
    "        \n",
    "        tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "        \n",
    "        i = 0\n",
    "        for result in estimator.predict(input_fn=read_dataset(args, mode=tf.estimator.ModeKeys.EVAL)):\n",
    "            print(result)\n",
    "            i += 1\n",
    "            if i == 20:\n",
    "                break\n",
    "    else:\n",
    "        \n",
    "        tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "        tfma.export.export_eval_savedmodel(\n",
    "            estimator=estimator,\n",
    "            export_dir_base='model_trained/eval/tfma/',\n",
    "            eval_input_receiver_fn=make_eval_input_fn(args)\n",
    "        )\n",
    "    \n",
    "    # export results\n",
    "#     if not os.path.exists('data/output'):\n",
    "#         os.mkdir('data/output')\n",
    "#     eval_preds = pd.DataFrame(list(estimator.predict(input_fn=read_dataset(args, tf.estimator.ModeKeys.EVAL))))\n",
    "#     probabilities = list(list(arr) for arr in eval_preds['probabilities']) # pandas is weird with how it stores arrays\n",
    "#     with tf.Session() as sess:\n",
    "#         eval_preds['probability'] = sess.run(tf.reduce_max(probabilities, reduction_indices=[1]))\n",
    "#     eval_preds['pred_' + config.LABEL_COL] = eval_preds['classes'].map(lambda x: x[0]) # predictions come in a list per row\n",
    "#     eval_preds = eval_preds[['pred_' + config.LABEL_COL, 'probability']]\n",
    "#     raw_eval_df = pd.concat([\n",
    "#         pd.read_csv(f, sep=config.DELIM, names=config.RENAMED_COLS)\n",
    "#         for f in tf.gfile.Glob('data/split/eval*.csv')], \n",
    "#         axis=0, ignore_index=True)\n",
    "#     cols = list(raw_eval_df.columns)\n",
    "#     cols.remove(config.LABEL_COL)\n",
    "#     raw_eval_df = raw_eval_df[cols + [config.LABEL_COL]]\n",
    "#     for col in ['pred_' + config.LABEL_COL, 'probability']:\n",
    "#         raw_eval_df[col] = eval_preds[col]\n",
    "#     raw_eval_df['wrong'] = (raw_eval_df['pred_' + config.LABEL_COL] != raw_eval_df[config.LABEL_COL]).astype(int)\n",
    "#     raw_eval_df.to_excel('data/output/eval_with_preds.xlsx', index=False)\n",
    "    \n",
    "    \n",
    "def gzip_reader_fn():\n",
    "    return tf.TFRecordReader(options=tf.python_io.TFRecordOptions(\n",
    "        compression_type=tf.python_io.TFRecordCompressionType.GZIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0l1scrAJtUQk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='{name_of_model}',\n",
    "    version='0.1',\n",
    "    author = '{name of author}',\n",
    "    author_email = '{email@example.com}',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='{Some description}',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train_data_paths',\n",
    "        help = 'GCS or local path to training data',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_batch_size',\n",
    "        help = 'Batch size for training steps',\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_batch_size',\n",
    "        help = 'Batch size for evaluation steps',\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        help = 'Epochs to run the training job for',\n",
    "        type = int,\n",
    "        default = 50\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_steps',\n",
    "        help = 'Number of steps to run evalution for at each checkpoint',\n",
    "        default = 10,\n",
    "        type = int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_data_paths',\n",
    "        help = 'GCS or local path to evaluation data',\n",
    "        required = True\n",
    "    )\n",
    "    # TensorFlow Transform args\n",
    "    parser.add_argument(\n",
    "        '--metadata_path',\n",
    "        help = 'GCS or local path to transformed metadata if using TFT',\n",
    "        default = '../../data/tft/metadata'\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = 'junk'\n",
    "    )\n",
    "    # Eval arguments\n",
    "    parser.add_argument(\n",
    "        '--eval_delay_secs',\n",
    "        help = 'How long to wait before running first evaluation',\n",
    "        default = 10,\n",
    "        type = int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min_eval_frequency',\n",
    "        help = 'Minimum number of training steps between evaluations',\n",
    "        default = 1,\n",
    "        type = int\n",
    "    )\n",
    "    # Model Specific arguments\n",
    "    parser.add_argument(\n",
    "        '--model_type',\n",
    "        help='Type of ML model, either \"linear\" or \"dnn\"',\n",
    "        default='linear',\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--embedding_type',\n",
    "        help='Embedding to use, one of \"nnlm\", \"universal-sentence-encoder\", \"elmo\"',\n",
    "        default='nnlm',\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='Learning rate',\n",
    "        default=0.01,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--hidden_units',\n",
    "        help='Hidden units of the DNN or (units of multi-layer) RNN model, separated by space e.g. \"128 64\"',\n",
    "        default='128 64',\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dropout',\n",
    "        help='Dropout rate (between 0 and 1)',\n",
    "        default=0.0,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--l1_regularization_strength',\n",
    "        help='L1 regularisation strength; controls how sparse the linear model will be',\n",
    "        default=0.01,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--l2_regularization_strength',\n",
    "        help='L2 regularisation strength; controls the magnitude of the weights in the linear model',\n",
    "        default=0.01,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--blocks',\n",
    "        help='No. of blocks in the sepCNN model. Good numbers to try are 1, 2, 4',\n",
    "        default=2,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--filters',\n",
    "        help='No. of conv filters within each layer in the sepCNN model. Good numbers to try are 8, 16, 32, 64, 128.',\n",
    "        default=32,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--kernel_size',\n",
    "        help='Kernel size of each conv filter in the sepCNN model. Good numbers to try are 3 and 5',\n",
    "        default=3,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--pool_size',\n",
    "        help='Pool size of each pooling layer in the sepCNN model',\n",
    "        default=3,\n",
    "        type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--debug',\n",
    "        help='use this while testing out if the model works',\n",
    "        default='False',\n",
    "        type=str\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop('job_dir', None)\n",
    "    arguments.pop('job-dir', None)\n",
    "\n",
    "    output_dir = arguments['model_dir']\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    try:\n",
    "        model.train_and_evaluate(arguments)\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/__init__.py\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964244], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996419], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964232], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964194], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996404], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964232], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996406], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964265], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964295], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996424], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964092], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964182], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964036], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964307], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964042], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964247], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996411], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996412], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996428], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996406], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996429], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964092], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964206], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996419], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964152], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964298], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996411], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964206], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964244], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964212], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996404], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964152], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964158], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964238], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964292], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996424], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964042], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964173], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964063], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964206], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964265], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996428], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964098], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964247], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964042], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964277], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964206], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964262], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964063], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996428], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996422], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996411], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964268], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964682], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964182], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964307], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996427], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996427], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964322], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964283], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964063], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964307], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964274], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964203], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964158], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964274], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964182], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964235], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964152], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996414], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996412], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964104], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964104], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299643], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964045], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964307], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964212], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964092], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964367], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964212], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996414], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964283], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996422], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996429], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964104], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964158], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964036], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964164], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299643], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996422], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964113], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996404], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996427], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964235], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964292], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964113], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996424], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964098], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964325], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964218], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964036], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964274], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964098], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964182], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964164], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996428], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996411], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964218], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996422], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964232], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964203], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996428], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964352], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996432], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964232], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964098], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964036], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996414], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996406], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964268], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964268], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964113], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964337], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964304], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964343], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964343], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964045], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964283], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964182], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964262], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996425], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964283], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964283], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996422], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996451], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964247], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964063], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964602], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996437], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964098], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996412], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964113], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964063], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964334], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996414], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964122], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996452], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964346], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964364], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996429], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964313], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964158], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964104], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964274], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996456], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964194], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964316], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964164], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964277], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964143], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964384], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964262], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964036], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964116], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964164], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996442], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996414], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964274], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964134], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964322], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964113], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964244], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964188], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964092], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964253], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996422], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964128], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964283], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996404], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964063], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964083], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964238], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964146], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964235], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996435], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964274], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964077], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996412], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299643], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964095], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996406], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996416], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964265], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964185], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996406], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964048], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996423], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964244], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996419], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996408], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996404], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299641], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964262], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964206], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964066], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964074], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996413], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996428], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964072], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964152], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996424], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996417], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996409], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964104], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964113], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996411], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964176], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964054], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964155], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996425], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996405], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964396], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996426], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.299642], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964057], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996407], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964137], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964086], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964215], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.2996415], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n",
      "{'probabilities': array([0.29964107], dtype=float32), 'prediction': array(['ham'], dtype=object)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiewpeng/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb0f151cd50>, '_model_dir': './model_trained', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow_transform/saved/input_fn_maker.py:560: read_batch_features (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:833: read_keyed_batch_features (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:542: read_keyed_batch_examples (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:423: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/input.py:197: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From trainer/model.py:362: __init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:449: shuffle_batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).shuffle(min_after_dequeue).batch(batch_size)`.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:550: queue_parsed_features (from tensorflow.contrib.learn.python.learn.learn_io.graph_io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.data.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Using /data/tfhub_modules to cache modules.\n",
      "2018-12-04 21:45:47.262531: W tensorflow/core/graph/graph_constructor.cc:1265] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-12-04 21:45:48.918083: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2018-12-04 21:45:49.030566: W tensorflow/core/framework/allocator.cc:122] Allocation of 498570752 exceeds 10% of system memory.\n",
      "2018-12-04 21:45:49.032757: W tensorflow/core/framework/allocator.cc:122] Allocation of 504687000 exceeds 10% of system memory.\n",
      "2018-12-04 21:45:49.032831: W tensorflow/core/framework/allocator.cc:122] Allocation of 504688000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./model_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.90406007, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 5 into ./model_trained/model.ckpt.\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/learn_io/graph_io.py:457: batch_join (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.interleave(...).batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-12-04 21:46:06.291083: W tensorflow/core/graph/graph_constructor.cc:1265] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-12-04-13:46:07\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model_trained/model.ckpt-5\n",
      "2018-12-04 21:46:07.168215: W tensorflow/core/framework/allocator.cc:122] Allocation of 498570752 exceeds 10% of system memory.\n",
      "2018-12-04 21:46:07.168294: W tensorflow/core/framework/allocator.cc:122] Allocation of 504687000 exceeds 10% of system memory.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-12-04-13:46:11\n",
      "INFO:tensorflow:Saving dict for global step 5: accuracy = 0.8671875, global_step = 5, loss = 0.814353\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: ./model_trained/model.ckpt-5\n",
      "WARNING:tensorflow:partially_apply_saved_transform is deprecated.  Use the transform_raw_features method of the TFTrandformOutput class instead.\n",
      "2018-12-04 21:46:11.547873: W tensorflow/core/graph/graph_constructor.cc:1265] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
      "WARNING:tensorflow:Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
      "value: \"\\n\\013\\n\\tConst_2:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from ./model_trained/model.ckpt-5\n",
      "WARNING:tensorflow:From /home/jiewpeng/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py:1044: calling add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:Assets written to: ./model_trained/export/exporter/temp-1543931171/assets\n",
      "INFO:tensorflow:SavedModel written to: ./model_trained/export/exporter/temp-1543931171/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.8454635.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "2018-12-04 21:46:22.618944: W tensorflow/core/graph/graph_constructor.cc:1265] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./model_trained/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD\n",
    "rm -rf model_trained\n",
    "python -m trainer.task \\\n",
    "    --train_data_paths='./data/tft/train*' \\\n",
    "    --eval_data_paths='./data/tft/eval*' \\\n",
    "    --model_dir='./model_trained' \\\n",
    "    --num_epochs=50 \\\n",
    "    --train_batch_size=256 \\\n",
    "    --eval_batch_size=256 \\\n",
    "    --metadata_path='./data/tft/metadata' \\\n",
    "    \\\n",
    "    --model_type='sepcnn' \\\n",
    "    --embedding_type='wikiwords250' \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --hidden_units='128 64' \\\n",
    "    --dropout=0.4 \\\n",
    "    --l1_regularization_strength=0.01 \\\n",
    "    --l2_regularization_strength=0.01 \\\n",
    "    --blocks=2 \\\n",
    "    --filters=32 \\\n",
    "    --kernel_size=3 \\\n",
    "    --pool_size=3 \\\n",
    "    --debug='False'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Model\n",
    "Better to run this in an actual terminal rather than here, so you can continue running other stuff.\n",
    "\n",
    "1. Replace 'model_trained' with whatever OUTPUT_DIR you have specified\n",
    "1. Replace 'exporter' with whatever you specified in `tf.estimator.LatestExporter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tensorflow_model_server \\\n",
    "    --rest_api_port=9000 \\\n",
    "    --model_base_path=${PWD}/model_trained/export/exporter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "The REST API can be called using the following signature: `http:{URI}:{PORT}/v1/models/{MODEL_NAME}[/versions/{VERSION}]:{VERB}`\n",
    "\n",
    "where\n",
    "\n",
    "- MODEL_NAME is \"default\" if no model name is specified when exporting the model\n",
    "- Specifying the version is optional\n",
    "- VERB is one of 'classify', 'regress', 'predict'. For serving, you should be using 'predict'\n",
    "- signature_name should be 'predict' when serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting debug.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile debug.json\n",
    "{\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"text\": \"FreeMsg Hey there darling its been 3 weeks now and no word back! Id like some fun you up for it still? Tb ok! XxX std chgs to send, 1.50 to rcv\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"The quick brown fox jumps over a lazy dog.\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [\n",
      "        {\n",
      "            \"prediction\": [\"ham\"],\n",
      "            \"probabilities\": [5.07454e-17]\n",
      "        },\n",
      "        {\n",
      "            \"prediction\": [\"ham\"],\n",
      "            \"probabilities\": [5.37494e-14]\n",
      "        }\n",
      "    ]\n",
      "}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: v\n",
      "\r",
      "100   558  100   227  100   331   1923   2805 --:--:-- --:--:-- --:--:--  4728\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl v -H \"Content-Type: application/json\" -X POST \\\n",
    "    http://localhost:9000/v1/models/default:predict \\\n",
    "    -d @debug.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03-model.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
