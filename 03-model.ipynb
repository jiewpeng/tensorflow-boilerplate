{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejuHKQ9UfSMJ"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tensorflow-transform\n",
    "pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('trainer'):\n",
    "    os.mkdir('trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSkew8KnU3qJ"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "from __future__ import print_function, division, absolute_import # python 2 compatibility\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as tflearn\n",
    "import tensorflow.contrib.metrics as metrics\n",
    "from tensorflow_transform.saved import input_fn_maker, saved_transform_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "import tensorflow_hub as hub\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "import os\n",
    "print(tf.__version__)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYAQsX-rVK3H"
   },
   "outputs": [],
   "source": [
    "REGION = 'asia-east1'\n",
    "BUCKET = '{BUCKET}'\n",
    "PROJECT = '{PROJECT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EBEdbhATWPmL"
   },
   "source": [
    "# Cloud Setup\n",
    "This section is required only if running on cloud (ML Engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHu_5_dqrssE"
   },
   "outputs": [],
   "source": [
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ia9n5ZtEWFks"
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbkA9Vd_tKzU"
   },
   "source": [
    "# Set up Model as a Package\n",
    "\n",
    "## Pre-requisites\n",
    "Data is assumed to have been prepared in the `TFRecords` format with GZIP compression. This gets us the best performance and scalability compared to csv files. The conversion of `csv` to `TFRecords` should be done in the previous notebook, `02-tf_transform.ipynb`\n",
    "\n",
    "## Package Setup\n",
    "We need to set up our model as a package for training and serving.\n",
    "\n",
    "- `model.py` provides the code for data inputs and the model itself\n",
    "- `setup.py` provides metadata about the package\n",
    "- `task.py` sets up the package to be used from the command line, with arguments that specify hyperparameters to the model as well as GCP resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C952m8_sWKJf"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/model.py --append\n",
    "\n",
    "\n",
    "CSV_COLUMNS = ['spam', 'text']\n",
    "LABEL_COLUMN = 'spam'\n",
    "LABEL_VOCABULARY = ['ham', 'spam']\n",
    "N_CLASSES = len(LABEL_VOCABULARY)\n",
    "DEFAULTS = [['spam'], [\"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, 1.50 to rcv\"]]\n",
    "INPUT_COLUMNS = [\n",
    "    tf.placeholder(tf.string, name='text')\n",
    "]\n",
    "\n",
    "\n",
    "def build_estimator(model_dir, model_type, embedding_type, learning_rate,\n",
    "                    hidden_units, dropout,\n",
    "                    l1_regularization_strength, l2_regularization_strength):\n",
    "  \n",
    "    if embedding_type == 'nnlm':\n",
    "        module_url = 'https://tfhub.dev/google/nnlm-en-dim128/1'\n",
    "    elif embedding_type == 'universal-sentence-encoder':\n",
    "        module_url = 'https://tfhub.dev/google/universal-sentence-encoder/2'\n",
    "    elif embedding_type == 'elmo':\n",
    "        module_url = 'https://tfhub.dev/google/elmo/2'\n",
    "    elif embedding_type is None:\n",
    "        pass\n",
    "    else:\n",
    "        raise InputError('Embedding type must be one of \"nnlm\", \"universal-sentence-encoder\", \"elmo\", None')\n",
    "    \n",
    "    if embedding_type is not None:\n",
    "        embedding = hub.text_embedding_column('text', module_url, trainable=False)\n",
    "        \n",
    "    feature_columns = embedding = [embedding]\n",
    "    \n",
    "    if model_type == 'linear':\n",
    "        return tf.estimator.LinearClassifier(\n",
    "            feature_columns=feature_columns,\n",
    "            n_classes=N_CLASSES,\n",
    "            label_vocabulary=LABEL_VOCABULARY,\n",
    "            model_dir=model_dir,\n",
    "            optimizer=tf.train.FtrlOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "                l1_regularization_strength=l1_regularization_strength,\n",
    "                l2_regularization_strength=l2_regularization_strength\n",
    "            )\n",
    "        )\n",
    "    elif model_type == 'dnn':\n",
    "        return tf.estimator.DNNClassifier(\n",
    "            feature_columns=feature_columns,\n",
    "            hidden_units=hidden_units,\n",
    "            n_classes=N_CLASSES,\n",
    "            label_vocabulary=LABEL_VOCABULARY,\n",
    "            model_dir=model_dir,\n",
    "            optimizer=tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "            ),\n",
    "            dropout=dropout\n",
    "        )\n",
    "    else:\n",
    "        raise InputErorr('Model type must be one of \"linear\" or \"dnn\"')\n",
    "        \n",
    "        \n",
    "# Serving input function\n",
    "def make_serving_input_fn_for_base64_json(args):\n",
    "    raw_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(args['metadata_path'], 'rawdata_metadata'))\n",
    "    transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "    return input_fn_maker.build_parsing_transforming_serving_input_receiver_fn(\n",
    "        raw_metadata,\n",
    "        transform_savedmodel_dir,\n",
    "        exclude_raw_keys=[LABEL_COLUMN]\n",
    "    )\n",
    "\n",
    "def make_serving_input_fn(args):\n",
    "    transform_savedmodel_dir = (\n",
    "        os.path.join(args['metadata_path'], 'transform_fn'))\n",
    "    \n",
    "    def _input_fn():\n",
    "        feature_placeholders = {\n",
    "            column_name: tf.placeholder(tf.string, [None]) for column_name in 'text'.split(',')\n",
    "        }\n",
    "        \n",
    "        _, features = saved_transform_io.partially_apply_saved_transform(\n",
    "            transform_savedmodel_dir,\n",
    "            feature_placeholders\n",
    "        )\n",
    "        return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "# training, eval and test input function\n",
    "def read_dataset(args, mode):\n",
    "    batch_size = args['train_batch_size']\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        input_paths = args['train_data_paths']\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        input_paths = args['eval_data_paths']\n",
    "    else:\n",
    "        input_paths = args['test_data_paths']\n",
    "    \n",
    "    transformed_metadata = metadata_io.read_metadata(\n",
    "        os.path.join(args['metadata_path'], 'transformed_metadata'))\n",
    "    \n",
    "    return input_fn_maker.build_training_input_fn(\n",
    "        metadata=transformed_metadata,\n",
    "        file_pattern = (input_paths[0] if len(input_paths) == 1 else input_paths),\n",
    "        training_batch_size=batch_size,\n",
    "        label_keys=[LABEL_COLUMN],\n",
    "        reader=gzip_reader_fn,\n",
    "        randomize_input=(mode == tf.estimator.ModeKeys.TRAIN),\n",
    "        num_epochs=(None if mode == tf.estimator.ModeKeys.TRAIN else 1)\n",
    "    )\n",
    "\n",
    "\n",
    "# create tf.estimator train and evaluate function\n",
    "def train_and_evaluate(args):\n",
    "    # modify according to build_estimator function\n",
    "    estimator = build_estimator(\n",
    "        args['model_dir'],\n",
    "        args['model_type'],\n",
    "        args['embedding_type'],\n",
    "        args['learning_rate'],\n",
    "        args['hidden_units'].split(' '),\n",
    "        args['dropout'],\n",
    "        args['l1_regularization_strength'],\n",
    "        args['l2_regularization_strength']\n",
    "    )\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=read_dataset(args, tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=args['train_steps']\n",
    "    )\n",
    "    \n",
    "    exporter = tf.estimator.LatestExporter('exporter', make_serving_input_fn(args))\n",
    "    \n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=read_dataset(args, tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        exporters=exporter\n",
    "    )\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    \n",
    "def gzip_reader_fn():\n",
    "    return tf.TFRecordReader(options=tf.python_io.TFRecordOptions(\n",
    "        compression_type=tf.python_io.TFRecordCompressionType.GZIP))\n",
    "\n",
    "\n",
    "def get_eval_metrics():\n",
    "    return {\n",
    "        'accuracy': tflearn.MetricSpec(metric_fn=metrics.streaming_accuracy),\n",
    "        'training/hptuning/metric': tflearn.MetricSpec(metric_fn=metrics.streaming_accuracy),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0l1scrAJtUQk"
   },
   "outputs": [],
   "source": [
    "%%writefile trainer/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='{name_of_model}',\n",
    "    version='0.1',\n",
    "    author = '{name of author}',\n",
    "    author_email = '{email@example.com}',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='{Some description}',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/task.py\n",
    "import traceback\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train_data_paths',\n",
    "        help = 'GCS or local path to training data',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_batch_size',\n",
    "        help = 'Batch size for training steps',\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_batch_size',\n",
    "        help = 'Batch size for evaluation steps',\n",
    "        type = int,\n",
    "        default = 512\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_steps',\n",
    "        help = 'Steps to run the training job for',\n",
    "        type = int,\n",
    "        default = 5000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_steps',\n",
    "        help = 'Number of steps to run evalution for at each checkpoint',\n",
    "        default = 10,\n",
    "        type = int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--eval_data_paths',\n",
    "        help = 'GCS or local path to evaluation data',\n",
    "        required = True\n",
    "    )\n",
    "    # TensorFlow Transform args\n",
    "    parser.add_argument(\n",
    "        '--metadata_path',\n",
    "        help = 'GCS or local path to transformed metadata if using TFT',\n",
    "        default = '../../data/tft/metadata'\n",
    "    )\n",
    "    # Training arguments\n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        help = 'GCS location to write checkpoints and export models',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = 'junk'\n",
    "    )\n",
    "    # Eval arguments\n",
    "    parser.add_argument(\n",
    "        '--eval_delay_secs',\n",
    "        help = 'How long to wait before running first evaluation',\n",
    "        default = 10,\n",
    "        type = int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--min_eval_frequency',\n",
    "        help = 'Minimum number of training steps between evaluations',\n",
    "        default = 1,\n",
    "        type = int\n",
    "    )\n",
    "    # Model Specific arguments\n",
    "    parser.add_argument(\n",
    "        '--model_type',\n",
    "        help='Type of ML model, either \"linear\" or \"dnn\"',\n",
    "        default='linear',\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--embedding_type',\n",
    "        help='Embedding to use, one of \"nnlm\", \"universal-sentence-encoder\", \"elmo\"',\n",
    "        default='universal-sentence-encoder',\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='Learning rate',\n",
    "        default=0.01,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--hidden_units',\n",
    "        help='Hidden units of the DNN model, separated by space e.g. \"128 64\"',\n",
    "        default='128 64 32',\n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dropout',\n",
    "        help='Dropout rate (between 0 and 1)',\n",
    "        default=0.0,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--l1_regularization_strength',\n",
    "        help='L1 regularisation strength; controls how sparse the linear model will be',\n",
    "        default=0.01,\n",
    "        type=float\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--l2_regularization_strength',\n",
    "        help='L2 regularisation strength; controls the magnitude of the weights in the linear model',\n",
    "        default=0.01,\n",
    "        type=float\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop('job_dir', None)\n",
    "    arguments.pop('job-dir', None)\n",
    "\n",
    "    output_dir = arguments['model_dir']\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    output_dir = os.path.join(\n",
    "        output_dir,\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    )\n",
    "\n",
    "    # Run the training job:\n",
    "    try:\n",
    "        model.train_and_evaluate(arguments)\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/__init__.py\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD\n",
    "rm -rf model_trained\n",
    "python -m trainer.task \\\n",
    "    --train_data_paths='./data/tft/train*' \\\n",
    "    --eval_data_paths='./data/tft/eval*' \\\n",
    "    --model_dir='./model_trained' \\\n",
    "    --train_steps=100 \\\n",
    "    --metadata_path='./data/tft/metadata' \\\n",
    "    \\\n",
    "    --model_type='linear' \\\n",
    "    --embedding_type='universal-sentence-encoder' \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --hidden_units='128 64 32' \\\n",
    "    --dropout=0.0 \\\n",
    "    --l1_regularization_strength=0.01 \\\n",
    "    --l2_regularization_strength=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03-model.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
