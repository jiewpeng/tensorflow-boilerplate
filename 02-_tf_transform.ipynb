{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-tf_transform.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "GIw5zgSkf7mh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip uninstall -y google-cloud-dataflow\n",
        "pip install --upgrade --force tensorflow_transform apache-beam[gcp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvYMdQBxgGp9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_transform.beam import impl as beam_impl\n",
        "import shutil\n",
        "import os\n",
        "import datetime\n",
        "import apache_beam as beam\n",
        "\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PCiL-sUKgUp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "REGION = 'asia-east1'\n",
        "BUCKET = '{BUCKET}'\n",
        "PROJECT = '{PROJECT}'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MXxbc63JgWkx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Cloud Setup\n",
        "This section is only required if running on cloud"
      ]
    },
    {
      "metadata": {
        "id": "wPdeNmBNnAPj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7LnF9NCdgcZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3wqf0DGrjKeR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create Dataset using tf.transform\n",
        "Much of the code is taken from [a notebook in Google's training data analyst repo](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/feateng/tftransform.ipynb).\n",
        "\n",
        "\n",
        "We will use `tf.transform` (which uses Apache Beam under the hood) for the data transformation pipeline. Doing this has several advantages:\n",
        "1. Same input pipeline can be used for batch and streaming data (just need to change the `beam.io.ReadFromText` line to whichever other file/DB connection).\n",
        "1. Same pipeline code can be used regardless of where it is running (on local machine, on GCP, on a Spark cluster).\n",
        "\n",
        "\n",
        "We need to have the following pre-requisites:\n",
        "1. Have a `requirements.txt` file for the Apache Beam pipeline\n",
        "1. Have a data filtering / validation function (here, it is the `is_valid` function)\n",
        "1. Have a data processing function (here, it is the `preprocess_tft` function)\n",
        "\n",
        "\n",
        "We can then define the pipeline:\n",
        "1. Import required packages for the pipeline\n",
        "1. Prepare the filesystem by deleting leftover files\n",
        "1. Set up the options for the pipeline\n",
        "1. Create metadata about the inputs (input columns and their datatypes) and save it\n",
        "1. Create a pipeline with the desired runner (e.g. `DirectRunner` or `DataflowRunner`)\n",
        "1. Transform raw training data\n",
        "  1. Read in raw training data\n",
        "  1. Analyze and transform the raw training data (produces a transformed dataset **and** a transform function to use on the eval and test datasets)\n",
        "  1. Save the transformed data\n",
        "1. Transform eval and test data\n",
        "  1. Read in raw eval/test data\n",
        "  1. Transform raw eval/test data\n",
        "  1. Save transformed eval/test data\n",
        "1. Save transform function for serving\n",
        "\n",
        "Note: when you read in data, it becomes a `PCollection`. Combining a `PCollection` with `DatasetMetadata` makes it a dataset. A dataset is a `tf.transform` concept."
      ]
    },
    {
      "metadata": {
        "id": "6mSdUpcFoW0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "tensorflow\n",
        "tensorflow-transform\n",
        "tensorflow-hub\n",
        "apache-beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DfDQYYAli_Q4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_dataset_path(phase, on_cloud=False):\n",
        "  if on_cloud:\n",
        "    data_dir = 'gs://{bucket}/spam-classification/data/split'.format(bucket=BUCKET)\n",
        "  else:\n",
        "    data_dir = 'data/split'\n",
        "  \n",
        "  if phase == 'train':\n",
        "    dataset_dir = os.path.join(data_dir, 'train*.csv')\n",
        "  elif phase == 'eval':\n",
        "    dataset_dir = os.path.join(data_dir, 'eval*.csv')\n",
        "  else:\n",
        "    dataset_dir = os.path.join(data_dir, 'test*.csv')\n",
        "    \n",
        "  return dataset_dir\n",
        "    \n",
        "\n",
        "def is_valid(inputs):\n",
        "  try:\n",
        "    text = inputs['text']\n",
        "    spam = inputs['spam']\n",
        "    return (text in ('ham', 'spam')) and (len(text) > 0)\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "\n",
        "def preprocess_tft(inputs):\n",
        "  result = {}\n",
        "  result['text'] = tf.identity(inputs['text'])\n",
        "  result['spam'] = tf.identity(inputs['spam'])\n",
        "  return result\n",
        "\n",
        "\n",
        "def preprocess(on_cloud=False):\n",
        "  import datetime\n",
        "  import os\n",
        "  import tempfile\n",
        "  from apache_beam.io import tfrecordio\n",
        "  from tensorflow_transform.coders import example_proto_coder\n",
        "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "  from tensorflow_transform.tf_metadata import dataset_schema\n",
        "  from tensorflow_transform.beam import tft_beam_io\n",
        "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
        "  \n",
        "  job_name = 'preprocess-spam-dataset' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
        "  \n",
        "  if on_cloud:\n",
        "    print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
        "    OUTPUT_DIR = 'gs://{bucket}/spam-classification/data/tft'.format(bucket=BUCKET)\n",
        "    import subprocess\n",
        "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
        "  else:\n",
        "    import shutil\n",
        "    print('Launching local job ... hang on')\n",
        "    OUTPUT_DIR = './data/tft'\n",
        "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
        "    \n",
        "  options = {\n",
        "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "    'job_name': job_name,\n",
        "    'project': PROJECT,\n",
        "    'max_num_workers': 24,\n",
        "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
        "    'no_save_main_session': True,\n",
        "    'requirements_file': 'requirements.txt'\n",
        "  }\n",
        "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "  \n",
        "  if on_cloud:\n",
        "    RUNNER = 'DataflowRunner'\n",
        "  else:\n",
        "    RUNNER = 'DirectRunner'\n",
        "    \n",
        "  # set up metadata\n",
        "  raw_data_schema = {\n",
        "      colname: dataset_schema.ColumnSchema(\n",
        "          tf.string, \n",
        "          [], \n",
        "          dataset_schema.FixedColumnRepresentation()\n",
        "      ) \n",
        "      for colname in 'spam,text'.split(',')\n",
        "  }\n",
        "  # raw_data_schema.update({\n",
        "  #     colname: dataset_schema.ColumnSchema(\n",
        "  #         tf.float64, \n",
        "  #         [], \n",
        "  #         dataset_schema.FixedColumnRepresentation()\n",
        "  #     )\n",
        "  #     for colname in 'somecolname,anothercolname'.split(',')\n",
        "  #   })\n",
        "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
        "  \n",
        "  # run Beam\n",
        "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
        "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
        "      # save the raw data metadata\n",
        "      _ = (\n",
        "          raw_data_metadata\n",
        "          | 'write_input_metadata' >> tft_beam_io.WriteMetadata(\n",
        "              os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
        "              pipeline=p))\n",
        "      \n",
        "      # analyze and transform training\n",
        "      raw_train_data = (\n",
        "          p \n",
        "          | 'train_read' >> beam.io.ReadFromText(get_dataset_path(phase='train', on_cloud=on_cloud))\n",
        "          | 'train_filter' >> beam.Filter(is_valid))\n",
        "      \n",
        "      raw_train_dataset = (raw_train_data, raw_data_metadata)\n",
        "      transformed_train_dataset, transform_fn = (\n",
        "          raw_train_dataset | beam_impl.AnalyzeAndTransformDataset(preprocessing_fn=preprocess_tft))\n",
        "      transformed_train_data, transformed_metadata = transformed_train_dataset\n",
        "      \n",
        "      # write transformed training data\n",
        "      _ = (\n",
        "          transformed_train_data\n",
        "          | 'write_train_data' >> tfrecordio.WriteToTFRecord(\n",
        "              os.path.join(OUTPUT_DIR, 'train'),\n",
        "              file_name_suffix='.gz',\n",
        "              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
        "      \n",
        "      # transform eval data\n",
        "      raw_eval_data = (\n",
        "          p\n",
        "          | 'eval_read' >> beam.io.ReadFromText(get_dataset_path(phase='eval', on_cloud=on_cloud))\n",
        "          | 'eval_filter' >> beam.Filter(is_valid))\n",
        "      raw_eval_dataset = (raw_eval_data, raw_data_metadata)\n",
        "      transformed_eval_dataset = (\n",
        "          (raw_eval_dataset, transform_fn) \n",
        "          | 'transform_eval_data' >> beam_impl.TransformDataset())\n",
        "      transformed_eval_data, _ = transformed_eval_dataset\n",
        "      \n",
        "      # write eval data\n",
        "      _ = (\n",
        "          transformed_eval_data\n",
        "          | 'write_eval_data' >> tfrecordio.WriteToTFRecord(\n",
        "              os.path.join(OUTPUT_DIR, 'eval'),\n",
        "              file_name_suffix='.gz',\n",
        "              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
        "      \n",
        "      # transform test data\n",
        "      raw_test_data = (\n",
        "          p\n",
        "          | 'test_read' >> beam.io.ReadFromText(get_dataset_path(phase='test', on_cloud=on_cloud))\n",
        "          | 'test_filter' >> beam.Filter(is_valid))\n",
        "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
        "      transformed_test_dataset = (\n",
        "          (raw_test_dataset, transform_fn) \n",
        "          | 'transform_test_data' >> beam_impl.TransformDataset())\n",
        "      transformed_test_data, _ = transformed_test_dataset\n",
        "      \n",
        "      # write test data\n",
        "      _ = (\n",
        "          transformed_test_data\n",
        "          | 'write_test_data' >> tfrecordio.WriteToTFRecord(\n",
        "              os.path.join(OUTPUT_DIR, 'test'),\n",
        "              file_name_suffix='.gz',\n",
        "              coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema)))\n",
        "      \n",
        "      # write transform function for serving\n",
        "      _ = (\n",
        "          transform_fn\n",
        "          | 'write_transform_fn' >> transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata'))\n",
        "      )\n",
        "      \n",
        "preprocess(on_cloud=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}