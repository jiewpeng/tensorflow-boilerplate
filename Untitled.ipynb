{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import SeparableConv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "def module_fn():\n",
    "    MAX_SEQ_LEN = 5\n",
    "    base_module = hub.Module('https://tfhub.dev/google/nnlm-en-dim128/1')\n",
    "    \n",
    "    text_input = tf.placeholder(dtype=tf.string, shape=[None, 1])\n",
    "    reshaped_input = tf.reshape(text_input, [-1])\n",
    "    split = tf.strings.split(reshaped_input)\n",
    "    split = tf.sparse.to_dense(split, default_value='')\n",
    "    seq_len = tf.shape(split)[1]    \n",
    "    batch_size = tf.shape(split)[0]\n",
    "    split = tf.cond(\n",
    "        seq_len < MAX_SEQ_LEN,\n",
    "        lambda: tf.pad(split, [[0, 0], [0, MAX_SEQ_LEN - seq_len]], constant_values=''),\n",
    "        lambda: tf.slice(split, [0, 0], [batch_size, MAX_SEQ_LEN])\n",
    "    )\n",
    "    embeddings = tf.map_fn(base_module, split, dtype=tf.float32)\n",
    "    hub.add_signature(inputs=text_input, outputs=embeddings)\n",
    "\n",
    "my_module_spec = hub.create_module_spec(\n",
    "    module_fn,\n",
    ")\n",
    "\n",
    "sequence_embed = hub.Module(my_module_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Embed within Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpdYVhAx\n",
      "INFO:tensorflow:Using the Keras model provided.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Module' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e83a2dbb5cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#     print(model.predict(test_data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/keras.pyc\u001b[0m in \u001b[0;36mmodel_to_estimator\u001b[0;34m(keras_model, keras_model_path, custom_objects, model_dir, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,\n\u001b[0;32m--> 484\u001b[0;31m                                              config)\n\u001b[0m\u001b[1;32m    485\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     logging.warning('You are creating an Estimator from a Keras model manually '\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/keras.pyc\u001b[0m in \u001b[0;36m_save_first_checkpoint\u001b[0;34m(keras_model, custom_objects, config)\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       model = _clone_and_build_model(model_fn_lib.ModeKeys.TRAIN, keras_model,\n\u001b[0;32m--> 354\u001b[0;31m                                      custom_objects)\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0;31m# save to checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/keras.pyc\u001b[0m in \u001b[0;36m_clone_and_build_model\u001b[0;34m(mode, keras_model, custom_objects, features, labels)\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mcompile_clone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_clone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0min_place_reset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       optimizer_iterations=global_step)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/models.pyc\u001b[0m in \u001b[0;36mclone_and_build_model\u001b[0;34m(model, input_tensors, target_tensors, custom_objects, compile_clone, in_place_reset, optimizer_iterations)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_place_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/models.pyc\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_sequential_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_functional_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/models.pyc\u001b[0m in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Clone layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mnew_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mlayer_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.pyc\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m       \u001b[0mfunction_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m       \u001b[0mfunction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m       \u001b[0mfunction_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'function'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Module' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "def CustomSeperableConv1D(filters, kernel_size):\n",
    "    return SeparableConv1D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        depthwise_initializer='random_uniform'\n",
    "    )\n",
    "\n",
    "input_layer = Input(shape=(1,), dtype='string')\n",
    "embeddings = Lambda(sequence_embed, name='sequence_embed')(input_layer)\n",
    "dropout1 = Dropout(0.5)(embeddings)\n",
    "sepconv1a = CustomSeperableConv1D(32, 3)(dropout1)\n",
    "sepconv1b = CustomSeperableConv1D(32, 3)(sepconv1a)\n",
    "globalavgpool = GlobalAveragePooling1D()(sepconv1b)\n",
    "dropout2 = Dropout(0.5)(globalavgpool)\n",
    "logits = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "# model.summary()\n",
    "\n",
    "x = np.array(['the quick brown fox', 'fruit juice'])\n",
    "y = np.array([0, 1])\n",
    "test_data = np.array(['orange juice', 'apple juice', 'big bad lion'])[:, np.newaxis]\n",
    "init = [tf.global_variables_initializer(), tf.tables_initializer()]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    model.fit(x, y, epochs=50, verbose=0)\n",
    "    print(model.predict(x))\n",
    "    print(model.predict(test_data))\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Embed Outside Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomSeperableConv1D(filters, kernel_size):\n",
    "    return SeparableConv1D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        depthwise_initializer='random_uniform'\n",
    "    )\n",
    "\n",
    "input_layer = Input(shape=(5, 128), dtype=tf.float32, name='text')\n",
    "dropout1 = Dropout(0.5)(input_layer)\n",
    "sepconv1a = CustomSeperableConv1D(32, 3)(dropout1)\n",
    "sepconv1b = CustomSeperableConv1D(32, 3)(sepconv1a)\n",
    "globalavgpool = GlobalAveragePooling1D()(sepconv1b)\n",
    "dropout2 = Dropout(0.5)(globalavgpool)\n",
    "logits = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "# model.summary()\n",
    "\n",
    "# x = np.array(['the quick brown fox', 'fruit juice'])[:, np.newaxis]\n",
    "# y = np.array([0, 1])\n",
    "# test_data = np.array(['orange juice', 'apple juice', 'big bad lion'])[:, np.newaxis]\n",
    "# init = [tf.global_variables_initializer(), tf.tables_initializer()]\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     embeddings = sess.run(sequence_embed(x))\n",
    "#     model.fit(embeddings, y, epochs=50, verbose=0)\n",
    "#     print(model.predict(sess.run(sequence_embed(x))))\n",
    "#     print(model.predict(sess.run(sequence_embed(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpscSvA0\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f82304ba9d0>, '_model_dir': '/tmp/tmpscSvA0', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the quick brown fox' 'fruit juice']\n",
      "[[ 5  4  0  1 -1]\n",
      " [ 3  2 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(['the quick brown fox', 'fruit juice'])\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=200)\n",
    "tokenizer.fit_on_texts(x)\n",
    "words = tokenizer.word_index.keys()\n",
    "table = tf.contrib.lookup.string_to_index_table_from_tensor(np.array(words))\n",
    "\n",
    "init = [tf.global_variables_initializer(), tf.tables_initializer()]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    MAX_SEQ_LEN = 5\n",
    "    text_input = tf.constant(x)\n",
    "    reshaped_input = tf.reshape(text_input, [-1])\n",
    "    split = tf.strings.split(reshaped_input)\n",
    "    split = tf.sparse.to_dense(split, default_value='')\n",
    "    seq_len = tf.shape(split)[1]    \n",
    "    batch_size = tf.shape(split)[0]\n",
    "    split = tf.cond(\n",
    "        seq_len < MAX_SEQ_LEN,\n",
    "        lambda: tf.pad(split, [[0, 0], [0, MAX_SEQ_LEN - seq_len]], constant_values=''),\n",
    "        lambda: tf.slice(split, [0, 0], [batch_size, MAX_SEQ_LEN])\n",
    "    )\n",
    "    print(x)\n",
    "    print(sess.run(table.lookup(split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to create a one-shot iterator for a dataset. `Dataset.make_one_shot_iterator()` does not support datasets that capture stateful objects, such as a `Variable` or `LookupTable`. In these cases, use `Dataset.make_initializable_iterator()`. (Original error: Cannot capture a stateful node (name:string_to_index_17/hash_table, type:HashTableV2) by value.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-38c57d959e8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.pyc\u001b[0m in \u001b[0;36mmake_one_shot_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;34m\"capture stateful objects, such as a `Variable` or `LookupTable`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;34m\"In these cases, use `Dataset.make_initializable_iterator()`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \"(Original error: %s)\" % err)\n\u001b[0m\u001b[1;32m    204\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to create a one-shot iterator for a dataset. `Dataset.make_one_shot_iterator()` does not support datasets that capture stateful objects, such as a `Variable` or `LookupTable`. In these cases, use `Dataset.make_initializable_iterator()`. (Original error: Cannot capture a stateful node (name:string_to_index_17/hash_table, type:HashTableV2) by value.)"
     ]
    }
   ],
   "source": [
    "x = np.array(['the quick brown fox', 'fruit juice'])\n",
    "y = np.array([0, 1])\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=200)\n",
    "tokenizer.fit_on_texts(x)\n",
    "words = tokenizer.word_index.keys()\n",
    "table = tf.contrib.lookup.string_to_index_table_from_tensor(tf.constant(words))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(({'text': x}, y))\n",
    "def preprocess_fn(x, y):\n",
    "    split = tf.strings.split(tf.expand_dims(x['text'], axis=-1))\n",
    "    split = tf.sparse.to_dense(split, default_value='')\n",
    "    seq_len = tf.shape(split)[1]    \n",
    "    batch_size = tf.shape(split)[0]\n",
    "    split = tf.cond(\n",
    "        seq_len < MAX_SEQ_LEN,\n",
    "        lambda: tf.pad(split, [[0, 0], [0, MAX_SEQ_LEN - seq_len]], constant_values=''),\n",
    "        lambda: tf.slice(split, [0, 0], [batch_size, MAX_SEQ_LEN])\n",
    "    )\n",
    "    split = table.lookup(split)\n",
    "    return {'text': split}, y\n",
    "dataset = dataset.map(preprocess_fn)\n",
    "generator = dataset.make_one_shot_iterator()\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(generator.get_next()))\n",
    "    print(sess.run(generator.get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 32)          6400      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "separable_conv1d_46 (Separab (None, None, 32)          1152      \n",
      "_________________________________________________________________\n",
      "separable_conv1d_47 (Separab (None, None, 32)          1152      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_23  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,737\n",
      "Trainable params: 8,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1TUxNZ\n",
      "INFO:tensorflow:Using the Keras model provided.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"lambda/cond/Merge:0\", shape=(?, ?), dtype=string) must be from the same graph as Tensor(\"string_to_index_16/hash_table:0\", shape=(), dtype=resource).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-eb6062835f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#     print(model.predict(test_data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/keras.pyc\u001b[0m in \u001b[0;36mmodel_to_estimator\u001b[0;34m(keras_model, keras_model_path, custom_objects, model_dir, config)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     warm_start_path = _save_first_checkpoint(keras_model, custom_objects,\n\u001b[0;32m--> 484\u001b[0;31m                                              config)\n\u001b[0m\u001b[1;32m    485\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     logging.warning('You are creating an Estimator from a Keras model manually '\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/keras.pyc\u001b[0m in \u001b[0;36m_save_first_checkpoint\u001b[0;34m(keras_model, custom_objects, config)\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       model = _clone_and_build_model(model_fn_lib.ModeKeys.TRAIN, keras_model,\n\u001b[0;32m--> 354\u001b[0;31m                                      custom_objects)\n\u001b[0m\u001b[1;32m    355\u001b[0m       \u001b[0;31m# save to checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/keras.pyc\u001b[0m in \u001b[0;36m_clone_and_build_model\u001b[0;34m(mode, keras_model, custom_objects, features, labels)\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mcompile_clone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_clone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0min_place_reset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       optimizer_iterations=global_step)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/models.pyc\u001b[0m in \u001b[0;36mclone_and_build_model\u001b[0;34m(model, input_tensors, target_tensors, custom_objects, compile_clone, in_place_reset, optimizer_iterations)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_place_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/models.pyc\u001b[0m in \u001b[0;36mclone_model\u001b[0;34m(model, input_tensors)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_sequential_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_clone_functional_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/models.pyc\u001b[0m in \u001b[0;36m_clone_functional_model\u001b[0;34m(model, input_tensors)\u001b[0m\n\u001b[1;32m    157\u001b[0m           \u001b[0mcomputed_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m           output_tensors = generic_utils.to_list(layer(computed_tensor,\n\u001b[0;32m--> 159\u001b[0;31m                                                        **kwargs))\n\u001b[0m\u001b[1;32m    160\u001b[0m           \u001b[0mcomputed_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.pyc\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mask'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/keras/layers/core.pyc\u001b[0m in \u001b[0;36mtext_to_seq\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCustomSeperableConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/lookup_ops.pyc\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, keys, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m     with ops.name_scope(name, \"%s_Lookup\" % self._name,\n\u001b[1;32m    227\u001b[0m                         (self._table_ref, key_tensor,\n\u001b[0;32m--> 228\u001b[0;31m                          self._default_value)) as scope:\n\u001b[0m\u001b[1;32m    229\u001b[0m       values = gen_lookup_ops.lookup_table_find_v2(\n\u001b[1;32m    230\u001b[0m           self._table_ref, key_tensor, self._default_value, name=scope)\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6021\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6022\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6023\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6024\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6025\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   5662\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5663\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5664\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5665\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5666\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agdljp/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   5598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5599\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[0;32m-> 5600\u001b[0;31m                                                                 original_item))\n\u001b[0m\u001b[1;32m   5601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor(\"lambda/cond/Merge:0\", shape=(?, ?), dtype=string) must be from the same graph as Tensor(\"string_to_index_16/hash_table:0\", shape=(), dtype=resource)."
     ]
    }
   ],
   "source": [
    "x = np.array(['the quick brown fox', 'fruit juice'])\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=200)\n",
    "tokenizer.fit_on_texts(x)\n",
    "words = tokenizer.word_index.keys()\n",
    "table = tf.contrib.lookup.string_to_index_table_from_tensor(tf.constant(words))\n",
    "x = x[:, np.newaxis]\n",
    "y = np.array([0, 1])\n",
    "test_data = np.array(['orange juice', 'apple juice', 'big bad lion'])[:, np.newaxis]\n",
    "\n",
    "def text_to_seq(texts):\n",
    "    MAX_SEQ_LEN = 5\n",
    "    reshaped_input = tf.reshape(texts, [-1])\n",
    "    split = tf.strings.split(reshaped_input)\n",
    "    split = tf.sparse.to_dense(split, default_value='')\n",
    "    seq_len = tf.shape(split)[1]    \n",
    "    batch_size = tf.shape(split)[0]\n",
    "    split = tf.cond(\n",
    "        seq_len < MAX_SEQ_LEN,\n",
    "        lambda: tf.pad(split, [[0, 0], [0, MAX_SEQ_LEN - seq_len]], constant_values=''),\n",
    "        lambda: tf.slice(split, [0, 0], [batch_size, MAX_SEQ_LEN])\n",
    "    )\n",
    "    \n",
    "    return table.lookup(split)\n",
    "\n",
    "tf.data.Dataset.from_tensor_slices()\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    \n",
    ")\n",
    "\n",
    "def CustomSeperableConv1D(filters, kernel_size):\n",
    "    return SeparableConv1D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        padding='same', \n",
    "        activation='relu',\n",
    "        bias_initializer='random_uniform',\n",
    "        depthwise_initializer='random_uniform'\n",
    "    )\n",
    "\n",
    "input_layer = Input(shape=(5,), dtype=tf.float32, name='tokenized_seq')\n",
    "embeddings = Embedding(input_dim=200, output_dim=32, input_length=5)(text_sequences)\n",
    "dropout1 = Dropout(0.5)(embeddings)\n",
    "sepconv1a = CustomSeperableConv1D(32, 3)(dropout1)\n",
    "sepconv1b = CustomSeperableConv1D(32, 3)(sepconv1a)\n",
    "globalavgpool = GlobalAveragePooling1D()(sepconv1b)\n",
    "dropout2 = Dropout(0.5)(globalavgpool)\n",
    "logits = Dense(1, activation='sigmoid')(dropout2)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# init = [tf.global_variables_initializer(), tf.tables_initializer()]\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     model.fit(x, y, epochs=50, verbose=0)\n",
    "#     print(model.predict(x))\n",
    "#     print(model.predict(test_data))\n",
    "\n",
    "estimator = tf.keras.estimator.model_to_estimator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
